# **Improving Human Pose-Conditioned Generation: <br/> Fine-tuning ControlNet Models with Reinforcement Learning**
<p align="center">
  <img src="src/proposed_architecture.png" width="700" title="hover text">
</p>

**Improving Human Pose-Conditioned Generation: Fine-tuning ControlNet Models with Reinforcement Learning** is a novel fine-tuning framework that enhances the reflection of conditioning images by integrating reinforcement learning into ControlNet models.

## Methodology
Our proposed framework fine-tunes ControlNet with the DDPO strategy. Unlike the original DDPO applying reinforcement learning to the Stable Diffusion model, we applies RL to the ControlNet model to enable learning new conditioning images. Instead of using the latent spae loss for training like ControlNet, we modified this to a reward system and applied DDPO's multi-step MDP. Additionally, unlike DDPO, we focus on pose-based human image generation. For this, we use a pose accuracy reward in addition to DDPO's text alignment reward. 

### Reward
<p align="center">
  <img src="src/reward.png" width="500" title="hover text">
</p>

We propose two kinds of reward functions that allow improving image-human pose alignment while maintaining image-text alignment.

## Installation

## Training

### 1. Training Dataset
<p align="center">
  <img src="src/dataset.png" width="600" title="hover text">
</p>

You need at least 10K 1024*1024 source images and corresponding pose conditioning images and prompts for training. Pose conditioning images can be obtained by extracting poses from source images using the OpenPose model. For pose conditioning images, you can choose between OpenPose without hands and face & OpenPose with hands and face.

If the training dataset is ready, a jsonl file that contains all dataset information is required. Keys for the jsonl file have to be  "image", "condition_image" and "text". The example of the jsonl file is as follows:

```json
{"image": "source/image/path1", "condition_image": "conditioning/image/path1", "text": "prompt1"}
{"image": "source/image/path2", "condition_image": "conditioning/image/path2", "text": "prompt2"}
{"image": "source/image/path3", "condition_image": "conditioning/image/path3", "text": "prompt3"}
...

```

### 2. Models
#### Models used for training
- Stable Diffusion XL -> stabilityai/stable-diffusion-xl-base-1.0
- ControlNet Openose SDXL -> thibaud/controlnet-openpose-sdxl-1.0
- VAE SDXL -> madebyollin/sdxl-vae-fp16-fix

#### Models used for reward computation
- LLAVAl -> lava-hf/llava-v1.6-mistral-7b-hf
- OpenPose Detector -> lllyasviel/ControlNet
- DINO -> facebook/dinov2-base

### 3. Files to be replaced


### 4. Strat Training
You can simply train the model with **VLM + Keypoints Reward fuction** by executing the shell file shown below.

```bash
export MODEL_DIR="stabilityai/stable-diffusion-xl-base-1.0"
export OUTPUT_DIR="output/path"

accelerate launch --config_file "config.yaml" train_controlnet_sdxl.py \
 --pretrained_model_name_or_path=$MODEL_DIR \
 --pretrained_vae_model_name_or_path madebyollin/sdxl-vae-fp16-fix \
 --controlnet_model_name_or_path thibaud/controlnet-openpose-sdxl-1.0 \
 --output_dir=$OUTPUT_DIR \
 --resolution=1024 \
 --train_data_dir='train_jsonl_file/path' \
 --checkpointing_steps 50 \
 --train_batch_size=2 \
 --gradient_accumulation_steps=4 \
 --learning_rate 1e-6 \
 --num_train_inference_steps 50 \
 --image_log_dir "image_log/path" \ #to save sample images
 --enable_xformers_memory_efficient_attention \
 --set_grads_to_none \
 --mixed_precision="fp16" \
 --caption_column="text" \
 --conditioning_image_column="condition_image" \
 --use_8bit_adam \
 --num_train_epochs=3
```


## Evaluation

### Baseline Model
- ControlNet Openose SDXL -> thibaud/controlnet-openpose-sdxl-1.0

### Eval Dataset
Source images and its corresponding keypoints and prompts have to be prepared separately. Results from the trained model by our mehtod and the baseline model can be generated by executing the code below. A csv file consists of names of conditioning images and prompts from source images.

```bash
python "data_generation_trained_model.py" \
--csv "/csv/path" \
--condition_image_path "/pose_conditioning_image/path" \
--save_dir "/save/path" \
--resolution 1024 \
--controlnet_model "/trained_model_or_baseline_model/path"
```

### Evaluation
Evaluation codes are under ./eval.

#### - Pose Accuracy(OKS)
0. no installation required
1. calculate OKS
```
python ./oks/oks_score.py --original original/keypoints/path --target target/keypoints/path
```


#### - Text Alignment(CLIP)
0. install clip from github
```
pip install git+https://github.com/openai/CLIP.git
```
1. prepare for the text prompt, if needed
```
python ./clip/csv_to_file.py
```
2. calculate clip score : text/path would be ./multiple_text or ./single_text
```
python ./clip/clip_score.py image/path text/path
```

#### - Image Quality(LPIPS)
0. install lpips
```
pip install lpips
```
1. calculate lpips score
```
python ./lpips/lpips_score.py -d0 original/image/path_0 -d1 generated/image/path_1
```